\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand*{\memsetcounter}[2]{}
\catcode `"\active 
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\babel@aux{english}{}
\babel@aux{english}{}
\@writefile{lof}{\addvspace {10pt}}
\@writefile{lot}{\addvspace {10pt}}
\@writefile{toc}{\contentsline {chapter}{Acknowledgments}{ii}{chapter*.2}}
\@writefile{lof}{\addvspace {10pt}}
\@writefile{lot}{\addvspace {10pt}}
\@writefile{toc}{\contentsline {chapter}{Abstract}{iii}{chapter*.3}}
\@writefile{toc}{\vspace  {2em}}
\citation{doi:10.1111/j.0956-7976.2005.00782.x}
\@writefile{lof}{\addvspace {10pt}}
\@writefile{lot}{\addvspace {10pt}}
\@writefile{toc}{\contentsline {chapter}{\chapternumberline {1}Introduction}{1}{chapter.4}}
\newlabel{chap:Introduction}{{\M@TitleReference {1}{Introduction}}{1}{Introduction}{chapter.4}{}}
\newlabel{chap:Introduction@cref}{{[chapter][1][]1}{[1][1][]1}}
\@writefile{toc}{\contentsline {section}{\numberline {1.1}Motivation}{1}{section.5}}
\newlabel{sec:Motivation}{{\M@TitleReference {1.1}{Motivation}}{1}{Motivation}{section.5}{}}
\newlabel{sec:Motivation@cref}{{[section][1][1]1.1}{[1][1][]1}}
\@writefile{toc}{\contentsline {section}{\numberline {1.2}Problem Statement}{1}{section.6}}
\newlabel{sec:Problem Statement}{{\M@TitleReference {1.2}{Problem Statement}}{1}{Problem Statement}{section.6}{}}
\newlabel{sec:Problem Statement@cref}{{[section][2][1]1.2}{[1][1][]1}}
\@writefile{toc}{\contentsline {section}{\numberline {1.3}Research Question \& Contributions}{2}{section.8}}
\newlabel{sec:Research Question & Contributions}{{\M@TitleReference {1.3}{Research Question \& Contributions}}{2}{Research Question \& Contributions}{section.8}{}}
\newlabel{sec:Research Question & Contributions@cref}{{[section][3][1]1.3}{[1][2][]2}}
\@writefile{toc}{\contentsline {section}{\numberline {1.4}Thesis Structure}{2}{section.9}}
\newlabel{sec:Thesis Structure}{{\M@TitleReference {1.4}{Thesis Structure}}{2}{Thesis Structure}{section.9}{}}
\newlabel{sec:Thesis Structure@cref}{{[section][4][1]1.4}{[1][2][]2}}
\citation{Minsky:1974:FRK:889222,Davis1993WhatIA}
\citation{LiuZhiyuan:247,7358050}
\@writefile{lof}{\addvspace {10pt}}
\@writefile{lot}{\addvspace {10pt}}
\@writefile{toc}{\contentsline {chapter}{\chapternumberline {2}Background}{3}{chapter.10}}
\newlabel{chap:Background}{{\M@TitleReference {2}{Background}}{3}{Background}{chapter.10}{}}
\newlabel{chap:Background@cref}{{[chapter][2][]2}{[1][3][]3}}
\@writefile{toc}{\contentsline {section}{\numberline {2.1}Knowledge Graphs}{3}{section.11}}
\newlabel{sec:Knowledge Graph}{{\M@TitleReference {2.1}{Knowledge Graphs}}{3}{Knowledge Graphs}{section.11}{}}
\newlabel{sec:Knowledge Graph@cref}{{[section][1][2]2.1}{[1][3][]3}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.1}{\ignorespaces An example of a knowledge graph representing how countries are related geographically.\relax }}{3}{figure.caption.12}}
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{fig:kg-example}{{\M@TitleReference {2.1}{An example of a knowledge graph representing how countries are related geographically.\relax }}{3}{An example of a knowledge graph representing how countries are related geographically.\relax }{figure.caption.12}{}}
\newlabel{fig:kg-example@cref}{{[figure][1][2]2.1}{[1][3][]3}}
\@writefile{toc}{\contentsline {paragraph}{Bank knowledge graph: }{4}{section*.13}}
\@writefile{toc}{\contentsline {paragraph}{Social network knowledge graph: }{4}{section*.14}}
\@writefile{toc}{\contentsline {paragraph}{Online shop knowledge graph (figure~\ref  {fig:kg-online-shop}): }{4}{section*.15}}
\citation{10008954896}
\citation{6796948}
\@writefile{lof}{\contentsline {figure}{\numberline {2.2}{\ignorespaces  An example of evolving knowledge graph for the ``Online Shop`` case described above. The evolution of the graph is shown through the labels $T=t$ for some edges. Indeed, the clients are buying, and generally performing actions, over time and the structure of the graph is evolving accordingly at different time step. The external information linking these \textit  {Converse} shoes (made with jeans fabric) and a \textit  {Levi's 511} may come from an external database to automatically map product identifiers to materials and fabrics they are made of. \relax }}{5}{figure.caption.16}}
\newlabel{fig:kg-online-shop}{{\M@TitleReference {2.2}{ An example of evolving knowledge graph for the ``Online Shop`` case described above. The evolution of the graph is shown through the labels $T=t$ for some edges. Indeed, the clients are buying, and generally performing actions, over time and the structure of the graph is evolving accordingly at different time step. The external information linking these \textit  {Converse} shoes (made with jeans fabric) and a \textit  {Levi's 511} may come from an external database to automatically map product identifiers to materials and fabrics they are made of. \relax }}{5}{ An example of evolving knowledge graph for the ``Online Shop`` case described above. The evolution of the graph is shown through the labels $T=t$ for some edges. Indeed, the clients are buying, and generally performing actions, over time and the structure of the graph is evolving accordingly at different time step. The external information linking these \textit {Converse} shoes (made with jeans fabric) and a \textit {Levi's 511} may come from an external database to automatically map product identifiers to materials and fabrics they are made of. \relax }{figure.caption.16}{}}
\newlabel{fig:kg-online-shop@cref}{{[figure][2][2]2.2}{[1][5][]5}}
\@writefile{toc}{\contentsline {section}{\numberline {2.2}Recurrent Neural Network}{5}{section.17}}
\newlabel{sec:Recurrent Neural Network}{{\M@TitleReference {2.2}{Recurrent Neural Network}}{5}{Recurrent Neural Network}{section.17}{}}
\newlabel{sec:Recurrent Neural Network@cref}{{[section][2][2]2.2}{[1][5][]5}}
\newlabel{eq:rnn_hidden}{{\M@TitleReference {2.1}{Recurrent Neural Network}}{5}{Recurrent Neural Network}{equation.18}{}}
\newlabel{eq:rnn_hidden@cref}{{[equation][1][2]2.1}{[1][5][]5}}
\citation{doi:10.1162/neco.1997.9.8.1735}
\citation{DBLP:journals/corr/ChoMGBSB14}
\citation{DBLP:journals/corr/ChungGCB14}
\citation{DBLP:journals/corr/ChungGCB14}
\citation{DBLP:journals/corr/ChungGCB14}
\@writefile{lof}{\contentsline {figure}{\numberline {2.3}{\ignorespaces  The schematic views of an LSTM (\textit  {left}) and GRU (\textit  {right}) cell. On the left-hand cell, \emph  {i}, \emph  {o} and \emph  {f} mean respectively \emph  {input}, \emph  {output} and \emph  {forget} gates. Finally, on this same cell, \emph  {c} and \emph  {$\mathaccentV {tilde}07E{c}$} represents the old and new contents of the memory cell. For the right-hand cell, \emph  {r} and \emph  {z} are the reset and update gates while \emph  {h} and \emph  {$\mathaccentV {tilde}07E{h}$} are respectively the actual and new (or candidate) activation. Figure from~\cite  {DBLP:journals/corr/ChungGCB14}. \relax }}{6}{figure.caption.21}}
\newlabel{fig:cells}{{\M@TitleReference {2.3}{ The schematic views of an LSTM (\textit  {left}) and GRU (\textit  {right}) cell. On the left-hand cell, \emph  {i}, \emph  {o} and \emph  {f} mean respectively \emph  {input}, \emph  {output} and \emph  {forget} gates. Finally, on this same cell, \emph  {c} and \emph  {$\mathaccentV {tilde}07E{c}$} represents the old and new contents of the memory cell. For the right-hand cell, \emph  {r} and \emph  {z} are the reset and update gates while \emph  {h} and \emph  {$\mathaccentV {tilde}07E{h}$} are respectively the actual and new (or candidate) activation. Figure from~\cite  {DBLP:journals/corr/ChungGCB14}. \relax }}{6}{ The schematic views of an LSTM (\textit {left}) and GRU (\textit {right}) cell. On the left-hand cell, \emph {i}, \emph {o} and \emph {f} mean respectively \emph {input}, \emph {output} and \emph {forget} gates. Finally, on this same cell, \emph {c} and \emph {$\tilde {c}$} represents the old and new contents of the memory cell. For the right-hand cell, \emph {r} and \emph {z} are the reset and update gates while \emph {h} and \emph {$\tilde {h}$} are respectively the actual and new (or candidate) activation. Figure from~\cite {DBLP:journals/corr/ChungGCB14}. \relax }{figure.caption.21}{}}
\newlabel{fig:cells@cref}{{[figure][3][2]2.3}{[1][6][]6}}
\citation{LiuZhiyuan:247}
\citation{D16-1260,7527876,DBLP:journals/corr/TrivediDWS17,8047276}
\citation{DBLP:journals/corr/abs-1812-02289}
\@writefile{toc}{\contentsline {section}{\numberline {2.3}Evolving Knowledge Graph}{7}{section.22}}
\newlabel{sec:Evolving Knowledge Graph}{{\M@TitleReference {2.3}{Evolving Knowledge Graph}}{7}{Evolving Knowledge Graph}{section.22}{}}
\newlabel{sec:Evolving Knowledge Graph@cref}{{[section][3][2]2.3}{[1][7][]7}}
\@writefile{lof}{\addvspace {10pt}}
\@writefile{lot}{\addvspace {10pt}}
\@writefile{toc}{\contentsline {chapter}{\chapternumberline {3}Healthcare Application \& Dataset}{8}{chapter.23}}
\newlabel{chap:Dataset}{{\M@TitleReference {3}{Healthcare Application \& Dataset}}{8}{Healthcare Application \& Dataset}{chapter.23}{}}
\newlabel{chap:Dataset@cref}{{[chapter][3][]3}{[1][8][]8}}
\@writefile{toc}{\contentsline {section}{\numberline {3.1}Description}{8}{section.24}}
\newlabel{sec:Description}{{\M@TitleReference {3.1}{Description}}{8}{Description}{section.24}{}}
\newlabel{sec:Description@cref}{{[section][1][3]3.1}{[1][8][]8}}
\@writefile{lot}{\contentsline {table}{\numberline {3.1}{\ignorespaces Admissions\relax }}{9}{table.caption.27}}
\@writefile{lot}{\contentsline {table}{\numberline {3.2}{\ignorespaces Diagnoses\relax }}{9}{table.caption.28}}
\@writefile{lot}{\contentsline {table}{\numberline {3.3}{\ignorespaces Laboratory measurements\relax }}{10}{table.caption.29}}
\@writefile{lot}{\contentsline {table}{\numberline {3.4}{\ignorespaces Input events from CareVue (Electronic Medical Records system)\relax }}{10}{table.caption.30}}
\@writefile{lot}{\contentsline {table}{\numberline {3.5}{\ignorespaces Input events from Metavision (Electronic Medical Records system)\relax }}{10}{table.caption.31}}
\@writefile{lot}{\contentsline {table}{\numberline {3.6}{\ignorespaces Output events from CareVue and Metavision\relax }}{11}{table.caption.32}}
\@writefile{lot}{\contentsline {table}{\numberline {3.7}{\ignorespaces Prescriptions\relax }}{11}{table.caption.33}}
\@writefile{toc}{\contentsline {paragraph}{Limitation \& Scope}{11}{section*.34}}
\@writefile{toc}{\contentsline {section}{\numberline {3.2}Preparation}{12}{section.35}}
\newlabel{sec:Preparation}{{\M@TitleReference {3.2}{Preparation}}{12}{Preparation}{section.35}{}}
\newlabel{sec:Preparation@cref}{{[section][2][3]3.2}{[1][12][]12}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2.1}Pre-processing \& Cleaning}{12}{subsection.36}}
\@writefile{toc}{\contentsline {paragraph}{Admissions}{12}{section*.37}}
\@writefile{toc}{\contentsline {paragraph}{Diagnoses}{12}{section*.38}}
\@writefile{toc}{\contentsline {paragraph}{Laboratory measurements}{12}{section*.39}}
\@writefile{toc}{\contentsline {paragraph}{Input events from CareVue}{12}{section*.40}}
\@writefile{toc}{\contentsline {paragraph}{Input events from Metavision}{12}{section*.41}}
\@writefile{toc}{\contentsline {paragraph}{Output events from CareVue and Metavision}{12}{section*.42}}
\@writefile{toc}{\contentsline {paragraph}{Prescriptions}{12}{section*.43}}
\citation{Che2018}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2.2}Formation}{14}{subsection.44}}
\newlabel{sec:Formation}{{\M@TitleReference {3.2.2}{Formation}}{14}{Formation}{subsection.44}{}}
\newlabel{sec:Formation@cref}{{[subsection][2][3,2]3.2.2}{[1][14][]14}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2.3}Schematic Visualizations}{15}{subsection.45}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.1}{\ignorespaces \textbf  {Top row}: The visualization of a single admission, consisting of 6 chunks of duration $\tau $, each with their respective one-time or range events. \textbf  {Middle row}: The pre-processed and cleaned admission, range events have been converted to one-time events. \textbf  {Bottom row}: The final version of the admission that will be fed to the downstream model. The admission is padded and events are sampled with $K=1$, thus we only have 1 event per type and per chunk of duration $\tau $. Chunks with less than $K=1$ events are also \textit  {zero padded} (not shown) to match the $K$ number of events per type and per chunk.\relax }}{15}{figure.caption.46}}
\@writefile{toc}{\contentsline {section}{\numberline {3.3}Statistics}{16}{section.47}}
\newlabel{sec:Statistics}{{\M@TitleReference {3.3}{Statistics}}{16}{Statistics}{section.47}{}}
\newlabel{sec:Statistics@cref}{{[section][3][3]3.3}{[1][16][]16}}
\@writefile{lot}{\contentsline {table}{\numberline {3.8}{\ignorespaces Dataset size and unique values for each table\relax }}{16}{table.caption.48}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.2}{\ignorespaces \textbf  {Top-left}: This histogram represents the number of diagnoses per admission at discharge for the top 50 ICD 9 codes, which is interesting to bear in mind in our setup of multi-class multi-classification. \textbf  {Top right}: This histogram represents the number of occurrences among the top 50 diagnoses, this is an indicator of the class imbalance or not. \textbf  {Bottom-left}: The cumulative distribution of diagnoses over admissions, we can see that a few diagnoses span more than 90\% of the admissions very rapidly. \textbf  {Bottom right}: The zoomed cumulative distribution of diagnoses over admissions, focusing on the top 200 most frequent diagnoses. We can see that we cover 95.1\% of admissions with the top 50 most frequent ICD9 codes.\relax }}{17}{figure.caption.49}}
\newlabel{fig:icd9-codes}{{\M@TitleReference {3.2}{\textbf  {Top-left}: This histogram represents the number of diagnoses per admission at discharge for the top 50 ICD 9 codes, which is interesting to bear in mind in our setup of multi-class multi-classification. \textbf  {Top right}: This histogram represents the number of occurrences among the top 50 diagnoses, this is an indicator of the class imbalance or not. \textbf  {Bottom-left}: The cumulative distribution of diagnoses over admissions, we can see that a few diagnoses span more than 90\% of the admissions very rapidly. \textbf  {Bottom right}: The zoomed cumulative distribution of diagnoses over admissions, focusing on the top 200 most frequent diagnoses. We can see that we cover 95.1\% of admissions with the top 50 most frequent ICD9 codes.\relax }}{17}{\textbf {Top-left}: This histogram represents the number of diagnoses per admission at discharge for the top 50 ICD 9 codes, which is interesting to bear in mind in our setup of multi-class multi-classification. \textbf {Top right}: This histogram represents the number of occurrences among the top 50 diagnoses, this is an indicator of the class imbalance or not. \textbf {Bottom-left}: The cumulative distribution of diagnoses over admissions, we can see that a few diagnoses span more than 90\% of the admissions very rapidly. \textbf {Bottom right}: The zoomed cumulative distribution of diagnoses over admissions, focusing on the top 200 most frequent diagnoses. We can see that we cover 95.1\% of admissions with the top 50 most frequent ICD9 codes.\relax }{figure.caption.49}{}}
\newlabel{fig:icd9-codes@cref}{{[figure][2][3]3.2}{[1][17][]17}}
\@writefile{lof}{\addvspace {10pt}}
\@writefile{lot}{\addvspace {10pt}}
\@writefile{toc}{\contentsline {chapter}{\chapternumberline {4}Baseline}{18}{chapter.50}}
\newlabel{chap:Baseline}{{\M@TitleReference {4}{Baseline}}{18}{Baseline}{chapter.50}{}}
\newlabel{chap:Baseline@cref}{{[chapter][4][]4}{[1][18][]18}}
\@writefile{toc}{\contentsline {section}{\numberline {4.1}Overview}{18}{section.51}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.1}{\ignorespaces Example admission (\textit  {without} zero padding), where chunk entities have a natural ordering and event entities do not.\relax }}{18}{figure.caption.52}}
\newlabel{fig:single-adm-graph}{{\M@TitleReference {4.1}{Example admission (\textit  {without} zero padding), where chunk entities have a natural ordering and event entities do not.\relax }}{18}{Example admission (\textit {without} zero padding), where chunk entities have a natural ordering and event entities do not.\relax }{figure.caption.52}{}}
\newlabel{fig:single-adm-graph@cref}{{[figure][1][4]4.1}{[1][18][]18}}
\citation{DBLP:journals/corr/HamiltonYL17}
\@writefile{toc}{\contentsline {section}{\numberline {4.2}Machine Learning Model}{19}{section.53}}
\@writefile{toc}{\contentsline {paragraph}{Max aggregator}{19}{section*.54}}
\newlabel{eq:max_agg}{{\M@TitleReference {4.1}{Max aggregator}}{19}{Max aggregator}{equation.55}{}}
\newlabel{eq:max_agg@cref}{{[equation][1][4]4.1}{[1][19][]19}}
\@writefile{toc}{\contentsline {paragraph}{Mean aggregator}{20}{section*.56}}
\newlabel{eq:max_agg}{{\M@TitleReference {4.2}{Mean aggregator}}{20}{Mean aggregator}{equation.57}{}}
\newlabel{eq:max_agg@cref}{{[equation][2][4]4.2}{[1][20][]20}}
\@writefile{toc}{\contentsline {paragraph}{Sum aggregator}{20}{section*.58}}
\newlabel{eq:max_agg}{{\M@TitleReference {4.3}{Sum aggregator}}{20}{Sum aggregator}{equation.59}{}}
\newlabel{eq:max_agg@cref}{{[equation][3][4]4.3}{[1][20][]20}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.2}{\ignorespaces An example of an aggregator on a certain event type (\textit  {blue}), the dashed arrow represent the type of aggregating function (i.e. sum, mean, max) applied element-wise (row-by-row on the figure). The output is a vector $\bm  {h}_{AGG} \in \mathbb  {R}^{a}$.\relax }}{20}{figure.caption.60}}
\newlabel{fig:agg-example}{{\M@TitleReference {4.2}{An example of an aggregator on a certain event type (\textit  {blue}), the dashed arrow represent the type of aggregating function (i.e. sum, mean, max) applied element-wise (row-by-row on the figure). The output is a vector $\bm  {h}_{AGG} \in \mathbb  {R}^{a}$.\relax }}{20}{An example of an aggregator on a certain event type (\textit {blue}), the dashed arrow represent the type of aggregating function (i.e. sum, mean, max) applied element-wise (row-by-row on the figure). The output is a vector $\bm {h}_{AGG} \in \mathbb {R}^{a}$.\relax }{figure.caption.60}{}}
\newlabel{fig:agg-example@cref}{{[figure][2][4]4.2}{[1][20][]20}}
\@writefile{toc}{\contentsline {section}{\numberline {4.3}Schematic Visualization}{21}{section.61}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.3}{\ignorespaces Visualization of the \emph  {Evolving Entity Encoder}, that passes each feature through their respective embedding, then aggregator and concatenate the results to be finally fed through the Recurrent Neural Network. This is application-agnostic and supports a varying number of features for each entity.\relax }}{21}{figure.caption.62}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.4}{\ignorespaces The \emph  {Evolving Entity Encoder} applied to our use-case. Hence, we are encoding an admission that consists in an evolving patient state to finally predict the diagnoses at discharge with an additional fully-connected layer.\relax }}{22}{figure.caption.63}}
\citation{Rotmensch2017}
\@writefile{lof}{\addvspace {10pt}}
\@writefile{lot}{\addvspace {10pt}}
\@writefile{toc}{\contentsline {chapter}{\chapternumberline {5}KG-RNN: Our Architecture}{23}{chapter.64}}
\newlabel{chap:KG-RNN}{{\M@TitleReference {5}{KG-RNN: Our Architecture}}{23}{KG-RNN: Our Architecture}{chapter.64}{}}
\newlabel{chap:KG-RNN@cref}{{[chapter][5][]5}{[1][23][]23}}
\@writefile{toc}{\contentsline {section}{\numberline {5.1}Overview}{23}{section.65}}
\@writefile{toc}{\contentsline {section}{\numberline {5.2}Weighted Knowledge Graph Construction}{23}{section.66}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.1}{\ignorespaces Final knowledge graph made of external and internal information that will be used in the experiments and discussions.\relax }}{24}{figure.caption.72}}
\newlabel{fig:kg-healthcare}{{\M@TitleReference {5.1}{Final knowledge graph made of external and internal information that will be used in the experiments and discussions.\relax }}{24}{Final knowledge graph made of external and internal information that will be used in the experiments and discussions.\relax }{figure.caption.72}{}}
\newlabel{fig:kg-healthcare@cref}{{[figure][1][5]5.1}{[1][24][]24}}
\citation{DBLP:journals/corr/HuL13,Leskovec:2006:SLG:1150402.1150479}
\citation{DBLP:journals/corr/abs-1806-01973}
\@writefile{toc}{\contentsline {section}{\numberline {5.3}Weighted Knowledge Graph Extraction}{25}{section.73}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.2}{\ignorespaces Example of potential \textit  {WPPR} scoring for the knowledge graph in figure~\ref  {fig:kg-healthcare}. If we set $M=1$, only ``Admission 2`` would be extracted, whereas if we set $M=2$ both admissions 1 and 2 would be extracted as neighbors of ``Admission 1``.\relax }}{25}{figure.caption.77}}
\newlabel{fig:kg-healthcare-extraction}{{\M@TitleReference {5.2}{Example of potential \textit  {WPPR} scoring for the knowledge graph in figure~\ref  {fig:kg-healthcare}. If we set $M=1$, only ``Admission 2`` would be extracted, whereas if we set $M=2$ both admissions 1 and 2 would be extracted as neighbors of ``Admission 1``.\relax }}{25}{Example of potential \textit {WPPR} scoring for the knowledge graph in figure~\ref {fig:kg-healthcare}. If we set $M=1$, only ``Admission 2`` would be extracted, whereas if we set $M=2$ both admissions 1 and 2 would be extracted as neighbors of ``Admission 1``.\relax }{figure.caption.77}{}}
\newlabel{fig:kg-healthcare-extraction@cref}{{[figure][2][5]5.2}{[1][25][]25}}
\@writefile{toc}{\contentsline {section}{\numberline {5.4}Graph Machine Learning}{26}{section.78}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.4.1}Schematic Visualizations}{27}{subsection.79}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.3}{\ignorespaces The general module of our architecture, where \textbf  {Evolving Entity Encoder} relies on work from previous chapter. The static properties and information from neighbors are concatenated and fed through an \emph  {Encoder}, consisting of a simple fully-connected layers to blend the concatenated information. All encoding vectors from neighbors are aggregated along the $M$ dimension and concatenated with the input entity. This vector is then further fed into a fully-connected layer, here hidden in the \emph  {Concatenate} block, to output the predictions.\relax }}{27}{figure.caption.80}}
\newlabel{fig:kg-rnn-general}{{\M@TitleReference {5.3}{The general module of our architecture, where \textbf  {Evolving Entity Encoder} relies on work from previous chapter. The static properties and information from neighbors are concatenated and fed through an \emph  {Encoder}, consisting of a simple fully-connected layers to blend the concatenated information. All encoding vectors from neighbors are aggregated along the $M$ dimension and concatenated with the input entity. This vector is then further fed into a fully-connected layer, here hidden in the \emph  {Concatenate} block, to output the predictions.\relax }}{27}{The general module of our architecture, where \textbf {Evolving Entity Encoder} relies on work from previous chapter. The static properties and information from neighbors are concatenated and fed through an \emph {Encoder}, consisting of a simple fully-connected layers to blend the concatenated information. All encoding vectors from neighbors are aggregated along the $M$ dimension and concatenated with the input entity. This vector is then further fed into a fully-connected layer, here hidden in the \emph {Concatenate} block, to output the predictions.\relax }{figure.caption.80}{}}
\newlabel{fig:kg-rnn-general@cref}{{[figure][3][5]5.3}{[1][27][]27}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.4}{\ignorespaces Similar to the previous schema, except that only static information from neighbors is encoded and squashed. Here the static information consist of only the diagnoses of these neighboring admissions.\relax }}{27}{figure.caption.81}}
\newlabel{fig:kg-rnn-healthcare}{{\M@TitleReference {5.4}{Similar to the previous schema, except that only static information from neighbors is encoded and squashed. Here the static information consist of only the diagnoses of these neighboring admissions.\relax }}{27}{Similar to the previous schema, except that only static information from neighbors is encoded and squashed. Here the static information consist of only the diagnoses of these neighboring admissions.\relax }{figure.caption.81}{}}
\newlabel{fig:kg-rnn-healthcare@cref}{{[figure][4][5]5.4}{[1][27][]27}}
\citation{DBLP:journals/corr/KingmaB14}
\@writefile{lof}{\addvspace {10pt}}
\@writefile{lot}{\addvspace {10pt}}
\@writefile{toc}{\contentsline {chapter}{\chapternumberline {6}Experiments}{28}{chapter.82}}
\newlabel{chap:Experiments}{{\M@TitleReference {6}{Experiments}}{28}{Experiments}{chapter.82}{}}
\newlabel{chap:Experiments@cref}{{[chapter][6][]6}{[1][28][]28}}
\@writefile{toc}{\contentsline {section}{\numberline {6.1}Setup}{28}{section.83}}
\newlabel{sec:Setup}{{\M@TitleReference {6.1}{Setup}}{28}{Setup}{section.83}{}}
\newlabel{sec:Setup@cref}{{[section][1][6]6.1}{[1][28][]28}}
\@writefile{toc}{\contentsline {section}{\numberline {6.2}Metrics}{30}{section.84}}
\newlabel{sec:Metrics}{{\M@TitleReference {6.2}{Metrics}}{30}{Metrics}{section.84}{}}
\newlabel{sec:Metrics@cref}{{[section][2][6]6.2}{[1][30][]30}}
\@writefile{toc}{\contentsline {paragraph}{F1 Score}{30}{section*.85}}
\@writefile{toc}{\contentsline {paragraph}{Area Under Receiver Operating Characteristic}{30}{section*.87}}
\@writefile{toc}{\contentsline {paragraph}{Macro and Micro Averages}{30}{section*.88}}
\@writefile{toc}{\contentsline {section}{\numberline {6.3}Deep Learning Hyper-parameter Tuning}{31}{section.89}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.3.1}Method}{31}{subsection.90}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.3.2}Search space}{31}{subsection.91}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.3.3}Results}{32}{subsection.92}}
\@writefile{toc}{\contentsline {paragraph}{Baseline}{32}{section*.93}}
\@writefile{toc}{\contentsline {paragraph}{KG-RNN}{32}{section*.94}}
\@writefile{toc}{\contentsline {section}{\numberline {6.4}Experiments}{33}{section.95}}
\newlabel{sec:Experiments}{{\M@TitleReference {6.4}{Experiments}}{33}{Experiments}{section.95}{}}
\newlabel{sec:Experiments@cref}{{[section][4][6]6.4}{[1][32][]33}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.4.1}Chunk length}{34}{subsection.96}}
\@writefile{toc}{\contentsline {paragraph}{Discussion}{34}{section*.98}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.4.2}Admission length}{35}{subsection.99}}
\@writefile{toc}{\contentsline {paragraph}{Discussion}{35}{section*.101}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.4.3}Number of events per chunk}{36}{subsection.102}}
\@writefile{toc}{\contentsline {paragraph}{Discussion}{36}{section*.104}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.4.4}Number of neighbors}{37}{subsection.105}}
\@writefile{toc}{\contentsline {paragraph}{Discussion}{37}{section*.107}}
\@writefile{toc}{\contentsline {section}{\numberline {6.5}ICD9 Prediction}{38}{section.108}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.5.1}Quantitative analysis}{38}{subsection.109}}
\@writefile{lof}{\contentsline {figure}{\numberline {6.1}{\ignorespaces \textbf  {Left}: Receiver operating characteristic curve on the testing set for both the \emph  {baseline} and \emph  {KG-RNN}. \textbf  {Right}: Precision-recall curve on the testing set for both the \emph  {baseline} and \emph  {KG-RNN}.\relax }}{38}{figure.caption.110}}
\@writefile{toc}{\contentsline {paragraph}{Discussion}{39}{section*.112}}
\citation{2018arXivUMAP}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.5.2}Qualitative Analysis}{40}{subsection.116}}
\newlabel{subsec:Qualitative analysis}{{\M@TitleReference {6.5.2}{Qualitative Analysis}}{40}{Qualitative Analysis}{subsection.116}{}}
\newlabel{subsec:Qualitative analysis@cref}{{[subsection][2][6,5]6.5.2}{[1][40][]40}}
\@writefile{toc}{\contentsline {paragraph}{Embeddings}{40}{section*.117}}
\@writefile{toc}{\contentsline {paragraph}{Predictions}{41}{section*.118}}
\@writefile{lof}{\contentsline {figure}{\numberline {6.2}{\ignorespaces In this plot we can see that the neighbors provide the necessary information to \emph  {KG-RNN} to improve over the baseline. Indeed, this model does not detect any diagnostic, whereas KG-RNN is able to predict the second one with the help of its neighbor. Namely, we hypothesize that since its neighbor static information conveys the ICD9 code of interest (the second one), it helped pushing the confidence of KG-RNN upward, above the 0.5 decision threshold.\relax }}{41}{figure.caption.119}}
\@writefile{lof}{\contentsline {figure}{\numberline {6.3}{\ignorespaces In this plot, the input admission has 4 neighbors with different final diagnoses (static information), and we also notice interesting behavior of \emph  {KG-RNN} compared to the baseline. In this scenario, we see that the first neighbor helps pushing the confidence of the model over the decision threshold for the second diagnosed ICD9 code. However, we also see that \emph  {KG-RNN} is misled by the first two neighbors that are wrongly pushing ICD9 number thirty-one over the decision threshold. This highlights the importance of a good knowledge graph construction and neighbors extraction algorithm.\relax }}{43}{figure.caption.122}}
\@writefile{lof}{\addvspace {10pt}}
\@writefile{lot}{\addvspace {10pt}}
\@writefile{toc}{\contentsline {chapter}{\chapternumberline {7}Concluding Remarks}{44}{chapter.123}}
\newlabel{chap:Concluding Remarks}{{\M@TitleReference {7}{Concluding Remarks}}{44}{Concluding Remarks}{chapter.123}{}}
\newlabel{chap:Concluding Remarks@cref}{{[chapter][7][]7}{[1][44][]44}}
\@writefile{toc}{\contentsline {section}{\numberline {7.1}Conclusion}{44}{section.124}}
\newlabel{sec:Conclusion}{{\M@TitleReference {7.1}{Conclusion}}{44}{Conclusion}{section.124}{}}
\newlabel{sec:Conclusion@cref}{{[section][1][7]7.1}{[1][44][]44}}
\citation{DBLP:journals/corr/HamiltonYL17}
\@writefile{toc}{\contentsline {section}{\numberline {7.2}Future Work}{45}{section.125}}
\newlabel{sec:Future Work}{{\M@TitleReference {7.2}{Future Work}}{45}{Future Work}{section.125}{}}
\newlabel{sec:Future Work@cref}{{[section][2][7]7.2}{[1][45][]45}}
\@writefile{toc}{\contentsline {paragraph}{Knowledge Graph Extensions}{45}{section*.126}}
\@writefile{toc}{\contentsline {paragraph}{Events sampler}{45}{section*.127}}
\@writefile{toc}{\contentsline {paragraph}{Neighbors Weighting}{45}{section*.128}}
\@writefile{toc}{\contentsline {paragraph}{Smarter Hyper-Optimization}{45}{section*.129}}
\@writefile{toc}{\contentsline {paragraph}{Distance-based Aggregators}{45}{section*.130}}
\@writefile{toc}{\contentsline {paragraph}{Threshold Tuning}{45}{section*.131}}
\bibstyle{ieeetr}
\bibdata{references}
\@writefile{toc}{\contentsline {paragraph}{Chunks Overlap}{46}{section*.132}}
\@writefile{toc}{\contentsline {paragraph}{ICD9 Hierarchy}{46}{section*.133}}
\bibcite{doi:10.1111/j.0956-7976.2005.00782.x}{{1}{}{{}}{{}}}
\bibcite{Minsky:1974:FRK:889222}{{2}{}{{}}{{}}}
\bibcite{Davis1993WhatIA}{{3}{}{{}}{{}}}
\bibcite{LiuZhiyuan:247}{{4}{}{{}}{{}}}
\bibcite{7358050}{{5}{}{{}}{{}}}
\bibcite{10008954896}{{6}{}{{}}{{}}}
\bibcite{6796948}{{7}{}{{}}{{}}}
\bibcite{doi:10.1162/neco.1997.9.8.1735}{{8}{}{{}}{{}}}
\bibcite{DBLP:journals/corr/ChoMGBSB14}{{9}{}{{}}{{}}}
\bibcite{DBLP:journals/corr/ChungGCB14}{{10}{}{{}}{{}}}
\bibcite{D16-1260}{{11}{}{{}}{{}}}
\bibcite{7527876}{{12}{}{{}}{{}}}
\bibcite{DBLP:journals/corr/TrivediDWS17}{{13}{}{{}}{{}}}
\bibcite{8047276}{{14}{}{{}}{{}}}
\bibcite{DBLP:journals/corr/abs-1812-02289}{{15}{}{{}}{{}}}
\bibcite{Che2018}{{16}{}{{}}{{}}}
\bibcite{DBLP:journals/corr/HamiltonYL17}{{17}{}{{}}{{}}}
\bibcite{Rotmensch2017}{{18}{}{{}}{{}}}
\@writefile{toc}{\contentsline {chapter}{Bibliography}{47}{chapter*.134}}
\bibcite{DBLP:journals/corr/HuL13}{{19}{}{{}}{{}}}
\bibcite{Leskovec:2006:SLG:1150402.1150479}{{20}{}{{}}{{}}}
\bibcite{DBLP:journals/corr/abs-1806-01973}{{21}{}{{}}{{}}}
\bibcite{DBLP:journals/corr/KingmaB14}{{22}{}{{}}{{}}}
\bibcite{2018arXivUMAP}{{23}{}{{}}{{}}}
\providecommand\NAT@force@numbers{}\NAT@force@numbers
\memsetcounter{lastsheet}{52}
\memsetcounter{lastpage}{48}
